{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4SsuY8QW5Ur"
   },
   "source": [
    "# Goal: Implement a (Rudimentary) ResNet Block and Visualize Loss Landscapes\n",
    "\n",
    "In this demo, we will implement a simple (and incomplete) residual block from the seminal ResNet paper. We will then compare a VGG network with a ResNet network, and finally, we will visualize the loss landscape of ResNets with and without skip connections.\n",
    "\n",
    "- [ResNet Paper](https://arxiv.org/abs/1512.03385)\n",
    "- [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZHYbg_RZUgs"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Let's start by importing our favorite packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JU1oEXziW8jE",
    "outputId": "71466f20-0e06-4e5c-c46c-9489f79129c8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.nn.utils import (\n",
    "  parameters_to_vector as Params2Vec,\n",
    "  vector_to_parameters as Vec2Params\n",
    ")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Roar1i-JZVyN"
   },
   "source": [
    "# Simple (and Incomplete) Implementation of ResNet Building Block\n",
    "\n",
    "The image below describes the basic idea of the Residual Block proposed in the paper. Rather than directly processing an input feature $x$ through a series of linear and non-linear transformations, we add a skip connection after these transformations and add back in our input $x$.\n",
    "\n",
    "![](https://raw.githubusercontent.com/kvgarimella/dl-demos/main/imgs/residual_block.png)\n",
    "\n",
    "Residual blocks have become ubiqitous in deep networks, even in transformer-based networks and have enabled the training of rather deep networks (for example, thousands of layers deep). We will stick to the case of CNNs where the **weight layer** in the above image is a 2D convolutional layer. Another common rule of thumb is to see `BatchNorm` added directly after a `Conv` layer (as discussed in class). Since `BatchNorm` contains learned parameters, we can think of them as being part of our weight layer. The image below shows the skip connection in action in a 34-layer networks (note that adding a skip connection does not increase the number of trainable parameters).\n",
    "\n",
    "![](https://raw.githubusercontent.com/kvgarimella/dl-demos/main/imgs/resnet34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU2TTOS1Qmk7"
   },
   "source": [
    "We can see that each weight layer consists of a $3 \\times 3$ convolution. In this particular image, the number of channels is $64$ although this varies depending upon what stage of the network you are in (see Figure 3 from the paper).\n",
    "\n",
    "To build a simplified residual block, let's place some constriants on our system:\n",
    "\n",
    "- we will assume the input $x$ already has the same number of channels as expected and outputed by the convolution layers \n",
    "- we will assume weight layer means a convolutional followed by a batchnorm layer\n",
    "- we won't worry about stride (i.e. reducing the resolution of the image)\n",
    "\n",
    "## Step 1: Filling in the `__init__` function\n",
    "\n",
    "Let's build our residual block as a Torch Module. This means we need both an `__init__` and `forward` function. For the `__init__` function, we need two initialize two convolution layers, two batchnorm, and two ReLU layers. We will have an input parameter called `num_channels` which will be the number of channels we expect our input image to have, and we will keep the number of channels constant throughout our residual block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cuMlTW4-Rk6Q"
   },
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhBywgqiRwkL"
   },
   "source": [
    "Great, now we have our init function (for the most part) complete.\n",
    "\n",
    "## Step 2: A first attempt at `forward`\n",
    "Let's now take a stab at the forward pass. We will model it after our image of the residual block above. Crucially, we will also **print out the `shape` of the image after each weight layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ndwcx4oeXD1U"
   },
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward through the first weight layer \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        print(\"shape after first weight layer:\", out.shape)\n",
    "\n",
    "        # forward through the second weight layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        print(\"shape after second weight layer:\", out.shape)\n",
    "\n",
    "        # our skip connection: adding back in x after both weight layers  \n",
    "        out += x \n",
    "\n",
    "        out = self.relu2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ0yem_VSw6m"
   },
   "source": [
    "Okay, let's instantiate a random torch \"image\" and attempt to propagate through our residual block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FA2IO1apikxK",
    "outputId": "ea0293ee-7c36-4805-d216-00946a9fde34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "NUM_CHANNELS = 64\n",
    "x = torch.randn(1, NUM_CHANNELS, 50, 50)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS8e9obKS47p"
   },
   "source": [
    "We have an \"image\" of $B \\times C \\times H \\times W = 1 \\times 64 \\times 50 \\times 50$. Let's pass this through our residual block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "2Y8OsQX8TMNW",
    "outputId": "edc49578-0ad9-4df8-dbaf-285c8e86a0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after first weight layer: torch.Size([1, 64, 48, 48])\n",
      "shape after second weight layer: torch.Size([1, 64, 46, 46])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (46) must match the size of tensor b (50) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d36991503a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbuilding_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildingBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilding_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e4abc475b71>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# our skip connection: adding back in x after both weight layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (46) must match the size of tensor b (50) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "building_block = BuildingBlock(num_channels=NUM_CHANNELS)\n",
    "y = building_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wo3CpPfTWzk"
   },
   "source": [
    "**We should see an error message telling us that our tensor don't have the same size**. We also see that our image has shrunk in size. We started with an image of height (and width) of 50. But after the first Conv layer, our image is now of size 48. And after the second Conv layer, the image has a height and width of 46. In order to add back in $x$ after going through each weight layer, we need to ensure that the output of each Conv layer has the same height and width. We can do this with padding.\n",
    "\n",
    "## Step 3: Fixing our Implementation with Padding\n",
    "Let's add in padding for each Conv layer in our `__init__` function. We will just need to set `padding=1` for both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rsGl55YzUPz6"
   },
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, padding=1) # padding added\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_channels, out_channels=num_channels, kernel_size=3, padding=1) # padding added\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward through the first weight layer \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        print(\"shape after first weight layer:\", out.shape)\n",
    "\n",
    "        # forward through the second weight layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        print(\"shape after second weight layer:\", out.shape)\n",
    "\n",
    "        # our skip connection: adding back in x after both weight layers  \n",
    "        out += x \n",
    "\n",
    "        out = self.relu2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsTnJqblVBNK"
   },
   "source": [
    "Now, let's retry our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WROELp5VUgvl",
    "outputId": "989c1a3c-612a-4311-c3f2-bf91b73337b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 50, 50])\n",
      "shape after first weight layer: torch.Size([1, 64, 50, 50])\n",
      "shape after second weight layer: torch.Size([1, 64, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "NUM_CHANNELS = 64\n",
    "x = torch.randn(1, NUM_CHANNELS, 50, 50)\n",
    "print(x.shape)\n",
    "building_block = BuildingBlock(num_channels=NUM_CHANNELS)\n",
    "y = building_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pokgXNU5Ukz8"
   },
   "source": [
    "This time, our input was succesfully processed through our implementation of the residual block. Adding padding for both convolutions preserved the height and width of the image throughout the block. And there is our rudimentary implementation of a Residual Block! PyTorch will take care of backpropagation for sure. \n",
    "\n",
    "For our implementation, we didn't take care of the case when the input image has a different number of channels after the weight layer has been applied. We also always have `stride=1`. Let's compare our implementation to [Torch's implementation below](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L59):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AVtyvgQVPrr"
   },
   "source": [
    "```python\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1tKpa98VPnR"
   },
   "source": [
    "Outside of the `downsample` portion (which takes care of the edge case we described above), the forward pass looks quite similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFxmvAZbVyhL"
   },
   "source": [
    "## Comparison with VGG\n",
    "Let's now import [VGG13](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg13.html#torchvision.models.vgg13) and [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) from torchvision and use torchsummary to see some of the differences in the two networks. Both of these networks have roughly the same test accuracy on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KBjG32yLktCN"
   },
   "outputs": [],
   "source": [
    "vgg = torchvision.models.vgg13()\n",
    "r18 = torchvision.models.resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oRSxJ4RWchf"
   },
   "source": [
    "Let's use the summary package to print out some statistics of performing a forward pass on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJuLvAc7k3Mm",
    "outputId": "1ec00cab-89da-4156-c2a7-7b3471468187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 224, 224]           1,792\n",
      "              ReLU-2          [1, 64, 224, 224]               0\n",
      "            Conv2d-3          [1, 64, 224, 224]          36,928\n",
      "              ReLU-4          [1, 64, 224, 224]               0\n",
      "         MaxPool2d-5          [1, 64, 112, 112]               0\n",
      "            Conv2d-6         [1, 128, 112, 112]          73,856\n",
      "              ReLU-7         [1, 128, 112, 112]               0\n",
      "            Conv2d-8         [1, 128, 112, 112]         147,584\n",
      "              ReLU-9         [1, 128, 112, 112]               0\n",
      "        MaxPool2d-10           [1, 128, 56, 56]               0\n",
      "           Conv2d-11           [1, 256, 56, 56]         295,168\n",
      "             ReLU-12           [1, 256, 56, 56]               0\n",
      "           Conv2d-13           [1, 256, 56, 56]         590,080\n",
      "             ReLU-14           [1, 256, 56, 56]               0\n",
      "        MaxPool2d-15           [1, 256, 28, 28]               0\n",
      "           Conv2d-16           [1, 512, 28, 28]       1,180,160\n",
      "             ReLU-17           [1, 512, 28, 28]               0\n",
      "           Conv2d-18           [1, 512, 28, 28]       2,359,808\n",
      "             ReLU-19           [1, 512, 28, 28]               0\n",
      "        MaxPool2d-20           [1, 512, 14, 14]               0\n",
      "           Conv2d-21           [1, 512, 14, 14]       2,359,808\n",
      "             ReLU-22           [1, 512, 14, 14]               0\n",
      "           Conv2d-23           [1, 512, 14, 14]       2,359,808\n",
      "             ReLU-24           [1, 512, 14, 14]               0\n",
      "        MaxPool2d-25             [1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-26             [1, 512, 7, 7]               0\n",
      "           Linear-27                  [1, 4096]     102,764,544\n",
      "             ReLU-28                  [1, 4096]               0\n",
      "          Dropout-29                  [1, 4096]               0\n",
      "           Linear-30                  [1, 4096]      16,781,312\n",
      "             ReLU-31                  [1, 4096]               0\n",
      "          Dropout-32                  [1, 4096]               0\n",
      "           Linear-33                  [1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 133,047,848\n",
      "Trainable params: 133,047,848\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 198.87\n",
      "Params size (MB): 507.54\n",
      "Estimated Total Size (MB): 706.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vgg, (3,224,224), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pgk7vERlKca",
    "outputId": "4aefa28c-76ba-4272-b5de-ea90363a3e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2          [1, 64, 112, 112]             128\n",
      "              ReLU-3          [1, 64, 112, 112]               0\n",
      "         MaxPool2d-4            [1, 64, 56, 56]               0\n",
      "            Conv2d-5            [1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6            [1, 64, 56, 56]             128\n",
      "              ReLU-7            [1, 64, 56, 56]               0\n",
      "            Conv2d-8            [1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9            [1, 64, 56, 56]             128\n",
      "             ReLU-10            [1, 64, 56, 56]               0\n",
      "       BasicBlock-11            [1, 64, 56, 56]               0\n",
      "           Conv2d-12            [1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13            [1, 64, 56, 56]             128\n",
      "             ReLU-14            [1, 64, 56, 56]               0\n",
      "           Conv2d-15            [1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16            [1, 64, 56, 56]             128\n",
      "             ReLU-17            [1, 64, 56, 56]               0\n",
      "       BasicBlock-18            [1, 64, 56, 56]               0\n",
      "           Conv2d-19           [1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20           [1, 128, 28, 28]             256\n",
      "             ReLU-21           [1, 128, 28, 28]               0\n",
      "           Conv2d-22           [1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23           [1, 128, 28, 28]             256\n",
      "           Conv2d-24           [1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25           [1, 128, 28, 28]             256\n",
      "             ReLU-26           [1, 128, 28, 28]               0\n",
      "       BasicBlock-27           [1, 128, 28, 28]               0\n",
      "           Conv2d-28           [1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29           [1, 128, 28, 28]             256\n",
      "             ReLU-30           [1, 128, 28, 28]               0\n",
      "           Conv2d-31           [1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32           [1, 128, 28, 28]             256\n",
      "             ReLU-33           [1, 128, 28, 28]               0\n",
      "       BasicBlock-34           [1, 128, 28, 28]               0\n",
      "           Conv2d-35           [1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36           [1, 256, 14, 14]             512\n",
      "             ReLU-37           [1, 256, 14, 14]               0\n",
      "           Conv2d-38           [1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39           [1, 256, 14, 14]             512\n",
      "           Conv2d-40           [1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41           [1, 256, 14, 14]             512\n",
      "             ReLU-42           [1, 256, 14, 14]               0\n",
      "       BasicBlock-43           [1, 256, 14, 14]               0\n",
      "           Conv2d-44           [1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45           [1, 256, 14, 14]             512\n",
      "             ReLU-46           [1, 256, 14, 14]               0\n",
      "           Conv2d-47           [1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48           [1, 256, 14, 14]             512\n",
      "             ReLU-49           [1, 256, 14, 14]               0\n",
      "       BasicBlock-50           [1, 256, 14, 14]               0\n",
      "           Conv2d-51             [1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52             [1, 512, 7, 7]           1,024\n",
      "             ReLU-53             [1, 512, 7, 7]               0\n",
      "           Conv2d-54             [1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55             [1, 512, 7, 7]           1,024\n",
      "           Conv2d-56             [1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57             [1, 512, 7, 7]           1,024\n",
      "             ReLU-58             [1, 512, 7, 7]               0\n",
      "       BasicBlock-59             [1, 512, 7, 7]               0\n",
      "           Conv2d-60             [1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61             [1, 512, 7, 7]           1,024\n",
      "             ReLU-62             [1, 512, 7, 7]               0\n",
      "           Conv2d-63             [1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64             [1, 512, 7, 7]           1,024\n",
      "             ReLU-65             [1, 512, 7, 7]               0\n",
      "       BasicBlock-66             [1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67             [1, 512, 1, 1]               0\n",
      "           Linear-68                  [1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(r18, (3,224,224), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnLMuFw-Wh0d"
   },
   "source": [
    "Things to Note: \n",
    "\n",
    "- ResNet18 has $\\sim 10$ percent of the number of parameters that VGG13 has (see `Total params`)\n",
    "- ResNet18 takes up $\\sim 15$ percent of the total size when compared to VGG13 (see `Total Size (MB)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMAETjyrXagt"
   },
   "source": [
    "# Visualizing the Loss Landscape of ResNets\n",
    "\n",
    "In addition to the skip connections of ResNets enabling the training of deeper networks, they seem to also smoothen out the loss surfaces of these networks. See the first figure in the Visualizing Loss Landscape paper:\n",
    "\n",
    "![](https://raw.githubusercontent.com/kvgarimella/dl-demos/main/imgs/loss_landscape.png)\n",
    "\n",
    "At a high level, these loss landscapes are generated by:\n",
    "\n",
    "1. training a network on a particular dataset (CIFAR in this case)\n",
    "2. slightly perturbing the weight values in two different directions and observing the loss value over the entire dataset.\n",
    "\n",
    "This is computationally intensive so we will be using a provided visualization tool by the authors. Navigate to the following URL: [http://www.telesens.co/loss-landscape-viz/viewer.html](http://www.telesens.co/loss-landscape-viz/viewer.html). \n",
    "\n",
    "Try out the following configurations:\n",
    "(no short) means that there are **no skip connections** in the network. \n",
    "\n",
    "## 1. ResNet 20 (no short) and ResNet 20 (short)\n",
    "\n",
    "Do you see a difference in the two loss landscapes?\n",
    "\n",
    "## 2. ResNet 56 (no short) and ResNet 56 (short)\n",
    "\n",
    "## 3. ResNet 110 (no short) and ResNet 110 (short)\n",
    "\n",
    "What happens to the loss landscapes as the network grows deeper?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
